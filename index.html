<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>From Entropy to Epiplexity — Paper Seminar</title>

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>

<!-- Fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Noto+Sans+KR:wght@300;400;500;600;700;900&family=JetBrains+Mono:wght@400;500;600&family=Playfair+Display:ital,wght@0,700;0,800;0,900;1,700&display=swap" rel="stylesheet">

<style>
/* ═══════════════════════════════════════════
   DESIGN SYSTEM — Variables
   ═══════════════════════════════════════════ */
:root {
  --font-display: 'Playfair Display', Georgia, serif;
  --font-body: 'Inter', 'Noto Sans KR', -apple-system, BlinkMacSystemFont, sans-serif;
  --font-mono: 'JetBrains Mono', 'SF Mono', monospace;

  --bg: #fafafa;
  --bg-card: #ffffff;
  --bg-code: #f5f5f7;
  --bg-glass: rgba(255, 255, 255, 0.72);

  --text-primary: #1d1d1f;
  --text-secondary: #6e6e73;
  --text-tertiary: #999;
  --text-caption: #86868b;

  --border: #e5e5ea;
  --border-light: #f0f0f0;

  --accent-blue: #0071e3;
  --accent-blue-soft: #e8f4fd;
  --accent-purple: #7c3aed;
  --accent-purple-soft: #f0e7ff;
  --accent-pink: #ec4899;
  --accent-pink-soft: #fce7f3;
  --accent-green: #059669;
  --accent-green-soft: #d1fae5;
  --accent-orange: #ea580c;
  --accent-orange-soft: #fff7ed;
  --accent-red: #dc2626;

  --gradient-hero: linear-gradient(135deg, #0f0c29 0%, #1a1040 30%, #24243e 60%, #0f0c29 100%);
  --gradient-blue: linear-gradient(135deg, #0071e3, #5ac8fa);
  --gradient-purple: linear-gradient(135deg, #7c3aed, #a78bfa);
  --gradient-pink: linear-gradient(135deg, #ec4899, #f472b6);
  --gradient-green: linear-gradient(135deg, #059669, #34d399);

  --shadow-sm: 0 1px 3px rgba(0,0,0,0.04);
  --shadow-md: 0 4px 20px rgba(0,0,0,0.06);
  --shadow-lg: 0 12px 40px rgba(0,0,0,0.1);
  --shadow-xl: 0 20px 60px rgba(0,0,0,0.12);
  --shadow-glow-blue: 0 0 40px rgba(0,113,227,0.15);
  --shadow-glow-purple: 0 0 40px rgba(124,58,237,0.15);

  --radius-sm: 8px;
  --radius-md: 12px;
  --radius-lg: 16px;
  --radius-xl: 24px;

  --max-w: 860px;
  --transition: 0.3s cubic-bezier(0.4, 0, 0.2, 1);
}

/* ═══════════════════════════════════════════
   RESET & BASE
   ═══════════════════════════════════════════ */
*, *::before, *::after { margin: 0; padding: 0; box-sizing: border-box; }
html { scroll-behavior: smooth; -webkit-text-size-adjust: 100%; }

body {
  font-family: var(--font-body);
  background: var(--bg);
  color: var(--text-primary);
  line-height: 1.8;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  overflow-x: hidden;
}

/* ═══════════════════════════════════════════
   READING PROGRESS BAR
   ═══════════════════════════════════════════ */
.progress-bar {
  position: fixed;
  top: 0;
  left: 0;
  height: 3px;
  background: var(--gradient-blue);
  z-index: 9999;
  width: 0%;
  transition: width 0.1s linear;
}

/* ═══════════════════════════════════════════
   FLOATING NAV DOTS
   ═══════════════════════════════════════════ */
.nav-dots {
  position: fixed;
  right: 28px;
  top: 50%;
  transform: translateY(-50%);
  z-index: 100;
  display: flex;
  flex-direction: column;
  gap: 8px;
  opacity: 0;
  transition: opacity 0.4s ease;
}
.nav-dots.visible { opacity: 1; }
.nav-dots a {
  display: block;
  width: 7px;
  height: 7px;
  border-radius: 50%;
  background: var(--text-tertiary);
  transition: all var(--transition);
  position: relative;
}
.nav-dots a.active {
  background: var(--accent-blue);
  transform: scale(1.5);
  box-shadow: 0 0 8px rgba(0,113,227,0.4);
}
.nav-dots a .tooltip {
  position: absolute;
  right: 18px;
  top: 50%;
  transform: translateY(-50%);
  background: var(--text-primary);
  color: #fff;
  padding: 3px 8px;
  border-radius: 6px;
  font-size: 0.65rem;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
  transition: opacity 0.2s, transform 0.2s;
  transform: translateY(-50%) translateX(6px);
}
.nav-dots a:hover .tooltip {
  opacity: 1;
  transform: translateY(-50%) translateX(0);
}

@media (max-width: 1100px) {
  .nav-dots { display: none; }
}

/* ═══════════════════════════════════════════
   HERO COVER
   ═══════════════════════════════════════════ */
.hero {
  min-height: 100vh;
  display: flex;
  align-items: center;
  justify-content: center;
  background: var(--gradient-hero);
  position: relative;
  overflow: hidden;
  padding: 4rem 2rem;
}
.hero::before {
  content: '';
  position: absolute;
  width: 600px;
  height: 600px;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(0,113,227,0.15) 0%, transparent 70%);
  top: -100px;
  right: -100px;
  animation: float-orb 12s ease-in-out infinite;
}
.hero::after {
  content: '';
  position: absolute;
  width: 500px;
  height: 500px;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(124,58,237,0.12) 0%, transparent 70%);
  bottom: -80px;
  left: -80px;
  animation: float-orb 15s ease-in-out infinite reverse;
}
@keyframes float-orb {
  0%, 100% { transform: translate(0, 0) scale(1); }
  33% { transform: translate(30px, -20px) scale(1.05); }
  66% { transform: translate(-20px, 15px) scale(0.95); }
}

.hero .grid-overlay {
  position: absolute;
  inset: 0;
  background-image:
    linear-gradient(rgba(255,255,255,0.03) 1px, transparent 1px),
    linear-gradient(90deg, rgba(255,255,255,0.03) 1px, transparent 1px);
  background-size: 40px 40px;
  pointer-events: none;
}

.hero-content {
  position: relative;
  z-index: 10;
  text-align: center;
  max-width: 800px;
  animation: hero-enter 1.2s cubic-bezier(0.16, 1, 0.3, 1) forwards;
}
@keyframes hero-enter {
  from { opacity: 0; transform: translateY(40px); }
  to { opacity: 1; transform: translateY(0); }
}

.hero-badge {
  display: inline-flex;
  align-items: center;
  gap: 6px;
  background: rgba(255,255,255,0.08);
  backdrop-filter: blur(20px);
  -webkit-backdrop-filter: blur(20px);
  border: 1px solid rgba(255,255,255,0.1);
  padding: 6px 16px;
  border-radius: 100px;
  font-size: 0.78rem;
  font-weight: 500;
  color: rgba(255,255,255,0.7);
  margin-bottom: 2rem;
  letter-spacing: 0.5px;
}
.hero-badge .dot {
  width: 6px;
  height: 6px;
  border-radius: 50%;
  background: #34d399;
  animation: pulse 2s infinite;
}
@keyframes pulse {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.4; }
}

.hero h1 {
  font-family: var(--font-display);
  font-size: clamp(2.6rem, 6vw, 4.2rem);
  font-weight: 800;
  color: #fff;
  line-height: 1.15;
  letter-spacing: -0.02em;
  margin-bottom: 1.2rem;
}
.hero h1 .gradient-text-blue {
  background: linear-gradient(135deg, #60a5fa, #38bdf8, #a5f3fc);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}
.hero h1 .gradient-text-pink {
  background: linear-gradient(135deg, #f472b6, #e879f9, #c084fc);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}
.hero .subtitle {
  font-size: 1.1rem;
  color: rgba(255,255,255,0.5);
  font-weight: 300;
  max-width: 520px;
  margin: 0 auto 2.5rem;
  line-height: 1.7;
}

.hero-meta {
  display: flex;
  justify-content: center;
  gap: 1.5rem;
  flex-wrap: wrap;
  font-size: 0.82rem;
  color: rgba(255,255,255,0.4);
}
.hero-meta span {
  display: flex;
  align-items: center;
  gap: 6px;
}
.hero-meta .icon {
  font-size: 0.9rem;
}

.scroll-hint {
  position: absolute;
  bottom: 2.5rem;
  left: 50%;
  transform: translateX(-50%);
  z-index: 10;
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 8px;
  color: rgba(255,255,255,0.3);
  font-size: 0.72rem;
  letter-spacing: 2px;
  text-transform: uppercase;
  animation: scroll-bounce 2s ease-in-out infinite;
}
.scroll-hint .arrow {
  width: 20px;
  height: 30px;
  border: 1.5px solid rgba(255,255,255,0.3);
  border-radius: 10px;
  position: relative;
}
.scroll-hint .arrow::after {
  content: '';
  position: absolute;
  width: 3px;
  height: 6px;
  background: rgba(255,255,255,0.5);
  border-radius: 3px;
  top: 6px;
  left: 50%;
  transform: translateX(-50%);
  animation: scroll-dot 2s ease-in-out infinite;
}
@keyframes scroll-dot {
  0%, 100% { top: 6px; opacity: 1; }
  50% { top: 16px; opacity: 0.3; }
}
@keyframes scroll-bounce {
  0%, 100% { transform: translateX(-50%) translateY(0); }
  50% { transform: translateX(-50%) translateY(6px); }
}

/* ═══════════════════════════════════════════
   PAGE LAYOUT
   ═══════════════════════════════════════════ */
.page {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 8rem;
}

/* ═══════════════════════════════════════════
   TABLE OF CONTENTS
   ═══════════════════════════════════════════ */
.toc {
  margin: 4rem 0 3rem;
  padding: 2rem;
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-sm);
}
.toc-header {
  display: flex;
  align-items: center;
  gap: 8px;
  margin-bottom: 1.2rem;
}
.toc-header .icon { font-size: 1rem; }
.toc-header h3 {
  font-size: 0.72rem;
  text-transform: uppercase;
  letter-spacing: 2px;
  color: var(--text-tertiary);
  font-weight: 600;
}
.toc-list {
  display: grid;
  grid-template-columns: repeat(2, 1fr);
  gap: 2px;
}
@media (max-width: 600px) { .toc-list { grid-template-columns: 1fr; } }
.toc-list a {
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 6px 12px;
  border-radius: var(--radius-sm);
  text-decoration: none;
  color: var(--text-secondary);
  font-size: 0.82rem;
  font-weight: 450;
  transition: all var(--transition);
}
.toc-list a:hover {
  background: var(--bg-code);
  color: var(--text-primary);
  transform: translateX(4px);
}
.toc-num {
  font-family: var(--font-mono);
  font-size: 0.7rem;
  font-weight: 600;
  color: var(--text-tertiary);
  min-width: 22px;
}

/* ═══════════════════════════════════════════
   SCROLL ANIMATIONS
   ═══════════════════════════════════════════ */
.reveal {
  opacity: 0;
  transform: translateY(24px);
  transition: opacity 0.7s cubic-bezier(0.16, 1, 0.3, 1),
              transform 0.7s cubic-bezier(0.16, 1, 0.3, 1);
}
.reveal.visible {
  opacity: 1;
  transform: translateY(0);
}
.reveal-delay-1 { transition-delay: 0.1s; }
.reveal-delay-2 { transition-delay: 0.2s; }
.reveal-delay-3 { transition-delay: 0.3s; }
.reveal-delay-4 { transition-delay: 0.4s; }

/* ═══════════════════════════════════════════
   SECTION STYLES
   ═══════════════════════════════════════════ */
.section {
  margin-top: 5rem;
  position: relative;
}

.section-number {
  font-family: var(--font-mono);
  font-size: 0.68rem;
  font-weight: 600;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: var(--text-tertiary);
  display: flex;
  align-items: center;
  gap: 8px;
  margin-bottom: 0.8rem;
}
.section-number::after {
  content: '';
  flex: 1;
  height: 1px;
  background: var(--border);
  max-width: 60px;
}

.section-tag {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  padding: 3px 10px;
  border-radius: 100px;
  font-size: 0.68rem;
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 0.8px;
  margin-bottom: 0.6rem;
}
.tag-core { background: var(--accent-blue-soft); color: var(--accent-blue); }
.tag-paradox { background: var(--accent-pink-soft); color: var(--accent-pink); }
.tag-experiment { background: var(--accent-green-soft); color: var(--accent-green); }
.tag-application { background: var(--accent-orange-soft); color: var(--accent-orange); }
.tag-background { background: #f3f4f6; color: #6b7280; }

h2.section-title {
  font-family: var(--font-body);
  font-size: 1.75rem;
  font-weight: 800;
  letter-spacing: -0.03em;
  line-height: 1.3;
  margin-bottom: 0.5rem;
  color: var(--text-primary);
}

.section-subtitle {
  font-size: 0.95rem;
  color: var(--text-secondary);
  line-height: 1.7;
  margin-bottom: 2rem;
}

h3.block-title {
  font-size: 1.05rem;
  font-weight: 700;
  margin: 2rem 0 0.8rem;
  color: var(--text-primary);
  letter-spacing: -0.01em;
}

/* ═══════════════════════════════════════════
   CALLOUT BLOCKS (Glassmorphism)
   ═══════════════════════════════════════════ */
.callout {
  display: flex;
  gap: 12px;
  padding: 1.2rem 1.4rem;
  border-radius: var(--radius-md);
  margin: 1.2rem 0;
  font-size: 0.9rem;
  line-height: 1.8;
  border: 1px solid;
  backdrop-filter: blur(8px);
  -webkit-backdrop-filter: blur(8px);
}
.callout-icon {
  font-size: 1.15rem;
  flex-shrink: 0;
  margin-top: 2px;
}
.callout-blue { background: rgba(0, 113, 227, 0.04); border-color: rgba(0, 113, 227, 0.12); }
.callout-purple { background: rgba(124, 58, 237, 0.04); border-color: rgba(124, 58, 237, 0.12); }
.callout-pink { background: rgba(236, 72, 153, 0.04); border-color: rgba(236, 72, 153, 0.12); }
.callout-green { background: rgba(5, 150, 105, 0.04); border-color: rgba(5, 150, 105, 0.12); }
.callout-orange { background: rgba(234, 88, 12, 0.04); border-color: rgba(234, 88, 12, 0.12); }
.callout-gray { background: rgba(0, 0, 0, 0.02); border-color: var(--border); }
.callout-yellow { background: rgba(234, 179, 8, 0.04); border-color: rgba(234, 179, 8, 0.15); }
.callout-key {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-left: 4px solid var(--accent-blue);
  box-shadow: var(--shadow-md);
}

/* ═══════════════════════════════════════════
   CARDS
   ═══════════════════════════════════════════ */
.card-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 16px;
  margin: 1.5rem 0;
}
.card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  padding: 1.4rem;
  transition: all var(--transition);
  position: relative;
  overflow: hidden;
}
.card::before {
  content: '';
  position: absolute;
  top: 0; left: 0; right: 0;
  height: 3px;
  background: var(--card-accent, var(--border));
  transition: height var(--transition);
}
.card:hover {
  box-shadow: var(--shadow-md);
  transform: translateY(-2px);
  border-color: transparent;
}
.card:hover::before { height: 4px; }
.card h4 {
  font-size: 0.92rem;
  font-weight: 700;
  margin-bottom: 0.5rem;
  line-height: 1.4;
}
.card p, .card li {
  font-size: 0.86rem;
  color: var(--text-secondary);
  line-height: 1.7;
}

/* ═══════════════════════════════════════════
   FIGURES
   ═══════════════════════════════════════════ */
.figure {
  margin: 2rem 0;
  text-align: center;
}
.figure-frame {
  display: inline-block;
  background: #fff;
  border-radius: var(--radius-md);
  box-shadow: var(--shadow-md);
  overflow: hidden;
  transition: all var(--transition);
  cursor: pointer;
  border: 1px solid var(--border-light);
}
.figure-frame:hover {
  box-shadow: var(--shadow-lg);
  transform: scale(1.01);
}
.figure-frame img {
  display: block;
  max-width: 100%;
  height: auto;
}
.figure-caption {
  margin-top: 0.8rem;
  font-size: 0.8rem;
  color: var(--text-caption);
  line-height: 1.65;
  text-align: left;
  max-width: 90%;
  margin-left: auto;
  margin-right: auto;
}
.figure-caption strong {
  color: var(--text-secondary);
  font-weight: 600;
}

/* Figure Lightbox */
.lightbox {
  display: none;
  position: fixed;
  inset: 0;
  z-index: 10000;
  background: rgba(0,0,0,0.85);
  backdrop-filter: blur(10px);
  -webkit-backdrop-filter: blur(10px);
  align-items: center;
  justify-content: center;
  cursor: zoom-out;
  animation: lb-in 0.3s ease;
}
.lightbox.open { display: flex; }
@keyframes lb-in { from { opacity: 0; } to { opacity: 1; } }
.lightbox img {
  max-width: 90vw;
  max-height: 90vh;
  border-radius: var(--radius-md);
  box-shadow: 0 20px 60px rgba(0,0,0,0.5);
}
.lightbox-close {
  position: absolute;
  top: 24px; right: 24px;
  width: 40px; height: 40px;
  border-radius: 50%;
  background: rgba(255,255,255,0.15);
  border: none;
  cursor: pointer;
  color: #fff;
  font-size: 1.3rem;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: background 0.2s;
}
.lightbox-close:hover { background: rgba(255,255,255,0.25); }

/* ═══════════════════════════════════════════
   EQUATIONS (KaTeX)
   ═══════════════════════════════════════════ */
.equation-block {
  background: var(--bg-code);
  border: 1px solid var(--border-light);
  border-radius: var(--radius-md);
  padding: 1.4rem 1.6rem;
  margin: 1.4rem 0;
  overflow-x: auto;
  text-align: center;
}

/* ═══════════════════════════════════════════
   TWO COLUMN LAYOUT
   ═══════════════════════════════════════════ */
.two-col {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 2rem;
  align-items: start;
  margin: 1.5rem 0;
}
@media (max-width: 768px) { .two-col { grid-template-columns: 1fr; } }

/* ═══════════════════════════════════════════
   TABLE
   ═══════════════════════════════════════════ */
.data-table {
  width: 100%;
  border-collapse: separate;
  border-spacing: 0;
  margin: 1.2rem 0;
  font-size: 0.86rem;
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.data-table th {
  text-align: left;
  padding: 10px 16px;
  background: var(--bg-code);
  font-weight: 600;
  font-size: 0.78rem;
  color: var(--text-secondary);
  text-transform: uppercase;
  letter-spacing: 0.5px;
}
.data-table td {
  padding: 10px 16px;
  border-top: 1px solid var(--border-light);
}
.data-table tr:hover td { background: rgba(0,113,227,0.02); }

/* ═══════════════════════════════════════════
   HIGHLIGHTS & BADGES
   ═══════════════════════════════════════════ */
.hl { font-weight: 600; }
.hl-blue { color: var(--accent-blue); }
.hl-purple { color: var(--accent-purple); }
.hl-pink { color: var(--accent-pink); }
.hl-green { color: var(--accent-green); }
.hl-orange { color: var(--accent-orange); }
.hl-red { color: var(--accent-red); }

.inline-badge {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  font-size: 0.75rem;
  font-weight: 600;
  padding: 2px 8px;
  border-radius: 4px;
}

/* ═══════════════════════════════════════════
   LISTS
   ═══════════════════════════════════════════ */
.clean-list {
  list-style: none;
  padding: 0;
  margin: 0.6rem 0;
}
.clean-list li {
  position: relative;
  padding: 4px 0 4px 20px;
  font-size: 0.9rem;
  line-height: 1.75;
}
.clean-list li::before {
  content: '';
  position: absolute;
  left: 2px;
  top: 13px;
  width: 5px;
  height: 5px;
  border-radius: 50%;
  background: var(--text-tertiary);
}
.clean-list.accent-blue li::before { background: var(--accent-blue); }
.clean-list.accent-pink li::before { background: var(--accent-pink); }
.clean-list.accent-green li::before { background: var(--accent-green); }
.clean-list.accent-purple li::before { background: var(--accent-purple); }

/* ═══════════════════════════════════════════
   TOGGLE / ACCORDION
   ═══════════════════════════════════════════ */
details {
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  margin-bottom: 10px;
  background: var(--bg-card);
  transition: box-shadow var(--transition);
}
details:hover { box-shadow: var(--shadow-sm); }
details summary {
  padding: 12px 16px;
  cursor: pointer;
  font-weight: 600;
  font-size: 0.9rem;
  user-select: none;
  border-radius: var(--radius-md);
  transition: background 0.2s;
  list-style: none;
  display: flex;
  align-items: center;
  gap: 8px;
}
details summary::-webkit-details-marker { display: none; }
details summary::before {
  content: '>';
  font-family: var(--font-mono);
  font-size: 0.75rem;
  color: var(--text-tertiary);
  transition: transform 0.2s;
  display: inline-block;
}
details[open] summary::before { transform: rotate(90deg); }
details summary:hover { background: var(--bg-code); }
details[open] summary { border-bottom: 1px solid var(--border-light); border-radius: var(--radius-md) var(--radius-md) 0 0; }
details .detail-body { padding: 16px; font-size: 0.88rem; color: var(--text-secondary); line-height: 1.8; }

/* ═══════════════════════════════════════════
   QUOTE
   ═══════════════════════════════════════════ */
.quote {
  border-left: 3px solid var(--accent-purple);
  padding: 12px 0 12px 20px;
  margin: 1.2rem 0;
  font-size: 0.95rem;
  color: var(--text-secondary);
  font-style: italic;
  line-height: 1.75;
  background: linear-gradient(90deg, rgba(124,58,237,0.03), transparent);
  border-radius: 0 var(--radius-sm) var(--radius-sm) 0;
}
.quote-author {
  font-style: normal;
  font-size: 0.82rem;
  color: var(--text-tertiary);
  margin-top: 6px;
  font-weight: 500;
}

/* ═══════════════════════════════════════════
   VS COMPARISON
   ═══════════════════════════════════════════ */
.vs-box {
  display: grid;
  grid-template-columns: 1fr auto 1fr;
  gap: 12px;
  align-items: stretch;
  margin: 1.2rem 0;
}
.vs-side {
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  padding: 1.2rem;
  text-align: center;
  transition: all var(--transition);
  background: var(--bg-card);
}
.vs-side:hover { box-shadow: var(--shadow-md); }
.vs-divider {
  display: flex;
  align-items: center;
  font-size: 0.82rem;
  font-weight: 800;
  color: var(--text-tertiary);
}

/* ═══════════════════════════════════════════
   PROPERTY GRID
   ═══════════════════════════════════════════ */
.prop-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 12px;
  margin: 1.2rem 0;
}
.prop-item {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-md);
  padding: 1rem 1.2rem;
  transition: all var(--transition);
}
.prop-item:hover { box-shadow: var(--shadow-sm); border-color: var(--accent-blue); }
.prop-item h4 { font-size: 0.82rem; font-weight: 700; color: var(--text-primary); margin-bottom: 4px; }
.prop-item p { font-size: 0.82rem; color: var(--text-secondary); line-height: 1.6; }

/* ═══════════════════════════════════════════
   STEP INDICATOR
   ═══════════════════════════════════════════ */
.step-flow {
  display: flex;
  align-items: center;
  gap: 0;
  margin: 1.5rem 0;
  flex-wrap: wrap;
}
.step-item {
  flex: 1;
  min-width: 140px;
  text-align: center;
  padding: 1rem 0.5rem;
  position: relative;
}
.step-num {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border-radius: 50%;
  background: var(--accent-blue);
  color: #fff;
  font-size: 0.78rem;
  font-weight: 700;
  margin-bottom: 0.5rem;
}
.step-item h4 { font-size: 0.82rem; font-weight: 700; margin-bottom: 4px; }
.step-item p { font-size: 0.78rem; color: var(--text-secondary); line-height: 1.5; }
.step-arrow {
  flex-shrink: 0;
  font-size: 1.2rem;
  color: var(--text-tertiary);
  margin: 0 -8px;
}

/* ═══════════════════════════════════════════
   SECTION DIVIDER, PARAGRAPH, FOOTER, SCROLL-TOP
   ═══════════════════════════════════════════ */
.divider { border: none; margin: 0; height: 0; }
p { margin-bottom: 0.8rem; font-size: 0.92rem; }
strong { font-weight: 600; }

footer {
  border-top: 1px solid var(--border);
  padding: 3rem 2rem;
  text-align: center;
}
.footer-inner {
  max-width: var(--max-w);
  margin: 0 auto;
  display: flex;
  justify-content: space-between;
  align-items: center;
  flex-wrap: wrap;
  gap: 1rem;
}
.footer-left { font-size: 0.78rem; color: var(--text-tertiary); }
.footer-right { font-size: 0.72rem; color: var(--text-tertiary); font-family: var(--font-mono); }

.scroll-top {
  position: fixed;
  bottom: 2rem;
  right: 2rem;
  width: 42px;
  height: 42px;
  border-radius: 50%;
  background: var(--bg-card);
  border: 1px solid var(--border);
  cursor: pointer;
  font-size: 1rem;
  display: none;
  align-items: center;
  justify-content: center;
  box-shadow: var(--shadow-md);
  z-index: 100;
  transition: all var(--transition);
  color: var(--text-primary);
}
.scroll-top:hover { transform: translateY(-2px); box-shadow: var(--shadow-lg); }

@media (max-width: 1100px) {
  .scroll-top { right: 1rem; bottom: 1rem; }
}
</style>
</head>
<body>

<!-- Reading Progress Bar -->
<div class="progress-bar" id="progressBar"></div>

<!-- Floating Nav Dots -->
<nav class="nav-dots" id="navDots">
  <a href="#sec01" data-section="sec01"><span class="tooltip">Summary</span></a>
  <a href="#sec02" data-section="sec02"><span class="tooltip">Why It Matters</span></a>
  <a href="#sec03" data-section="sec03"><span class="tooltip">Three Paradoxes</span></a>
  <a href="#sec04" data-section="sec04"><span class="tooltip">Randomness</span></a>
  <a href="#sec05" data-section="sec05"><span class="tooltip">Sophistication</span></a>
  <a href="#sec06" data-section="sec06"><span class="tooltip">Def: T-model</span></a>
  <a href="#sec07" data-section="sec07"><span class="tooltip">Def: Epiplexity</span></a>
  <a href="#sec08" data-section="sec08"><span class="tooltip">Properties</span></a>
  <a href="#sec09" data-section="sec09"><span class="tooltip">Prequential</span></a>
  <a href="#sec10" data-section="sec10"><span class="tooltip">Requential</span></a>
  <a href="#sec11" data-section="sec11"><span class="tooltip">Comparison</span></a>
  <a href="#sec12" data-section="sec12"><span class="tooltip">P1: Theory</span></a>
  <a href="#sec13" data-section="sec13"><span class="tooltip">P1: ECA</span></a>
  <a href="#sec14" data-section="sec14"><span class="tooltip">P2: Theory</span></a>
  <a href="#sec15" data-section="sec15"><span class="tooltip">P2: Chess</span></a>
  <a href="#sec16" data-section="sec16"><span class="tooltip">P3: Induction</span></a>
  <a href="#sec17" data-section="sec17"><span class="tooltip">P3: Emergence</span></a>
  <a href="#sec18" data-section="sec18"><span class="tooltip">OOD</span></a>
  <a href="#sec19" data-section="sec19"><span class="tooltip">Scaling</span></a>
  <a href="#sec20" data-section="sec20"><span class="tooltip">Discussion</span></a>
</nav>

<!-- Lightbox -->
<div class="lightbox" id="lightbox">
  <button class="lightbox-close" onclick="closeLightbox()">&times;</button>
  <img id="lightboxImg" src="" alt="">
</div>

<!-- ═══════════════════════════════════════════
     HERO
     ═══════════════════════════════════════════ -->
<header class="hero">
  <div class="grid-overlay"></div>
  <div class="hero-content">
    <div class="hero-badge">
      <span class="dot"></span>
      Paper Seminar &middot; 2026
    </div>
    <h1>
      From <span class="gradient-text-blue">Entropy</span><br>
      to <span class="gradient-text-pink">Epiplexity</span>
    </h1>
    <p class="subtitle">
      Rethinking Information for<br>
      Computationally Bounded Intelligence
    </p>
    <div class="hero-meta">
      <span><span class="icon">&#128214;</span> Finzi*, Qiu*, Jiang*, Izmailov, Kolter, Wilson</span>
      <span><span class="icon">&#127979;</span> CMU & NYU</span>
      <span><span class="icon">&#128197;</span> arXiv:2601.03220</span>
    </div>
  </div>
  <div class="scroll-hint">
    <div class="arrow"></div>
    Scroll
  </div>
</header>

<!-- ═══════════════════════════════════════════
     PAGE CONTENT
     ═══════════════════════════════════════════ -->
<div class="page">

<!-- TOC -->
<nav class="toc reveal" id="top">
  <div class="toc-header">
    <span class="icon">&#128218;</span>
    <h3>Table of Contents</h3>
  </div>
  <div class="toc-list">
    <a href="#sec01"><span class="toc-num">01</span> 한 줄 요약</a>
    <a href="#sec02"><span class="toc-num">02</span> 왜 이 논문이 중요한가</a>
    <a href="#sec03"><span class="toc-num">03</span> 세 가지 역설</a>
    <a href="#sec04"><span class="toc-num">04</span> 랜덤이란 무엇인가?</a>
    <a href="#sec05"><span class="toc-num">05</span> Sophistication의 한계</a>
    <a href="#sec06"><span class="toc-num">06</span> 정의: Time-bounded Model</a>
    <a href="#sec07"><span class="toc-num">07</span> 정의: Epiplexity</a>
    <a href="#sec08"><span class="toc-num">08</span> 기본 성질 & 핵심 정리</a>
    <a href="#sec09"><span class="toc-num">09</span> 측정: Prequential Coding</a>
    <a href="#sec10"><span class="toc-num">10</span> 측정: Requential Coding</a>
    <a href="#sec11"><span class="toc-num">11</span> 두 방법 비교 & 스케일링</a>
    <a href="#sec12"><span class="toc-num">12</span> Paradox 1 이론</a>
    <a href="#sec13"><span class="toc-num">13</span> Paradox 1 실험 (ECA)</a>
    <a href="#sec14"><span class="toc-num">14</span> Paradox 2 이론</a>
    <a href="#sec15"><span class="toc-num">15</span> Paradox 2 실험 (Chess)</a>
    <a href="#sec16"><span class="toc-num">16</span> Paradox 3A: 귀납</a>
    <a href="#sec17"><span class="toc-num">17</span> Paradox 3B: 창발</a>
    <a href="#sec18"><span class="toc-num">18</span> 응용: OOD 일반화</a>
    <a href="#sec19"><span class="toc-num">19</span> 응용: 스케일링 & 데이터 선택</a>
    <a href="#sec20"><span class="toc-num">20</span> 정리 & 토론</a>
  </div>
</nav>


<!-- ══════════ 01. SUMMARY ══════════ -->
<section class="section" id="sec01">
  <div class="reveal">
    <div class="section-number">01 — Summary</div>
    <span class="section-tag tag-core">Core Idea</span>
    <h2 class="section-title">이 논문을 한 문장으로</h2>
  </div>
  <div class="callout callout-key reveal reveal-delay-1">
    <div class="callout-icon">&#128161;</div>
    <div>
      기존 정보이론(Shannon Entropy, Kolmogorov Complexity)은 <strong>"무한한 계산 능력"</strong>을 가진 관찰자를 전제하므로 현대 ML 현상을 설명하지 못한다. 이 논문은 <span class="hl hl-blue">계산 제약이 있는 관찰자</span>의 관점에서 데이터의 정보를 <span class="hl hl-purple">구조적 정보(Epiplexity, \(S_T\))</span>와 <span class="hl hl-pink">랜덤 정보(Time-bounded Entropy, \(H_T\))</span>로 분리하는 새로운 체계를 제안한다.
    </div>
  </div>
  <p class="reveal reveal-delay-2">"Model Selection"에서 <strong>"Data Selection"</strong>으로의 패러다임 전환을 위한 이론적 기반을 제공하며, 어떤 데이터가 더 유용한 학습 신호를 제공하는지를 측정하는 실용적 방법론도 함께 제시한다.</p>
</section>


<!-- ══════════ 02. WHY IT MATTERS ══════════ -->
<section class="section" id="sec02">
  <div class="reveal">
    <div class="section-number">02 — Why It Matters</div>
    <span class="section-tag tag-core">Motivation</span>
    <h2 class="section-title">왜 이 논문이 중요한가?</h2>
    <p class="section-subtitle">현대 AI 시스템의 핵심 질문들에 기존 이론이 답하지 못하는 이유</p>
  </div>

  <div class="two-col reveal reveal-delay-1">
    <div>
      <h3 class="block-title" style="margin-top: 0;">데이터 시대의 도래</h3>
      <p>현대 AI 연구의 성패는 <strong>아키텍처 선택</strong>보다 <strong>어떤 데이터로 학습하느냐</strong>에 더 크게 좌우된다. 모델 선택(model selection)에서 데이터 선택(data selection)으로 패러다임이 전환되고 있다.</p>
      <ul class="clean-list accent-blue">
        <li>고품질 인터넷 데이터의 고갈</li>
        <li>합성 데이터의 가치는 무엇인가?</li>
        <li>텍스트 사전학습이 왜 이미지보다 일반화가 좋은가?</li>
        <li>어떤 데이터 큐레이션 전략이 최적인가?</li>
      </ul>
    </div>
    <div>
      <h3 class="block-title" style="margin-top: 0;">AlphaZero 역설</h3>
      <p><span class="hl hl-blue">AlphaZero</span>는 체스 규칙(간단한 결정론적 프로그램)과 자기 대국만으로 초인적 전략을 학습하여 <strong>수 메가바이트</strong>의 가중치에 저장했다.</p>
      <div class="callout callout-pink" style="margin-top: 0.8rem;">
        <div class="callout-icon">&#10067;</div>
        <div>기존 정보이론에 따르면, 결정론적 변환은 정보를 증가시킬 수 없다. 그렇다면 AlphaZero가 "학습한 정보"는 어디서 온 것인가?</div>
      </div>
    </div>
  </div>

  <div class="callout callout-green reveal reveal-delay-2">
    <div class="callout-icon">&#10003;</div>
    <div><strong>이 논문의 해답:</strong> 기존 이론은 <strong>무한한 계산 능력</strong>의 관찰자를 가정한다. 계산 제약(computation bound)을 도입하면, "정보"의 정의가 달라지고 위 질문들에 자연스러운 답을 줄 수 있다.</div>
  </div>
</section>


<!-- ══════════ 03. THREE PARADOXES ══════════ -->
<section class="section" id="sec03">
  <div class="reveal">
    <div class="section-number">03 — Three Paradoxes</div>
    <span class="section-tag tag-paradox">Paradox</span>
    <h2 class="section-title">세 가지 역설: 기존 이론의 한계</h2>
    <p class="section-subtitle">Shannon 정보이론과 Kolmogorov 복잡도로는 설명하기 어려운 현상들</p>
  </div>

  <div class="card-grid reveal reveal-delay-1">
    <div class="card" style="--card-accent: var(--accent-blue);">
      <h4>Paradox 1: 정보는 결정론적 변환으로 증가할 수 없다?</h4>
      <p><strong>이론:</strong> Data Processing Inequality: \(I(f(X); W) \leq I(X; W)\). Kolmogorov: \(K(f(x)) \leq K(x) + K(f) + c\).</p>
      <p><strong>현실:</strong> CSPRNG은 짧은 시드에서 긴 랜덤 시퀀스를 "생성"하고, AlphaZero는 규칙에서 전략을 "생성"하며, 합성 데이터는 모델 성능을 향상시킨다.</p>
    </div>
    <div class="card" style="--card-accent: var(--accent-purple);">
      <h4>Paradox 2: 정보는 순서에 의존하지 않는다?</h4>
      <p><strong>이론:</strong> \(H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X)\). Kolmogorov의 symmetry of information도 동일.</p>
      <p><strong>현실:</strong> LLM은 영어를 왼쪽→오른쪽으로 모델링할 때 역순보다 더 잘 압축한다. 소인수분해는 한 방향이 압도적으로 어렵다.</p>
    </div>
    <div class="card" style="--card-accent: var(--accent-pink);">
      <h4>Paradox 3: 우도 모델링은 분포 매칭일 뿐이다?</h4>
      <p><strong>이론:</strong> \(\arg\min_P \mathbb{E}[-\log P(X)] = Q\) (데이터 생성 분포). 모델은 생성 과정보다 더 배울 수 없다.</p>
      <p><strong>현실:</strong> Conway's Game of Life의 간단한 규칙에서 글라이더/진동자 등 <strong>창발적 구조</strong>가 나타나고, 추리소설 모델은 저자보다 복잡한 <strong>귀납 추론</strong>을 학습한다.</p>
    </div>
  </div>

  <div class="callout callout-key reveal reveal-delay-2">
    <div class="callout-icon">&#128273;</div>
    <div><strong>공통 원인:</strong> 세 역설 모두 기존 이론이 <strong>"관찰자의 계산 능력에 제한이 없다"</strong>고 가정하기 때문에 발생한다. 계산 제약을 도입하면 \(f\)와 \(f^{-1}\)의 비대칭성이 나타나고, 역설이 해소된다.</div>
  </div>
</section>


<!-- ══════════ 04. WHAT IS RANDOM? ══════════ -->
<section class="section" id="sec04">
  <div class="reveal">
    <div class="section-number">04 — Background</div>
    <span class="section-tag tag-background">Background</span>
    <h2 class="section-title">랜덤이란 무엇인가?</h2>
    <p class="section-subtitle">관찰자의 계산 능력에 따라 "랜덤"의 정의가 달라진다</p>
  </div>

  <details class="reveal reveal-delay-1" open>
    <summary>Level 1: Shannon 정보 — 확률 분포 기반</summary>
    <div class="detail-body">
      <p>확률 변수 \(X\)의 자기 정보량 = \(-\log P(x)\). 엔트로피 \(H(X) = \mathbb{E}[\log 1/P(X)]\).</p>
      <p><strong>한계:</strong> 결정론적 객체(특정 문자열)의 정보를 정의할 수 없다. \(1111\ldots\)과 \(10011101\ldots\)이 동일 확률이면 동일 정보로 취급되지만, 직관적으로 후자가 더 "랜덤"하다.</p>
    </div>
  </details>
  <details class="reveal reveal-delay-2" open>
    <summary>Level 2: Martin-Lof 랜덤 — 모든 알고리즘이 예측 불가</summary>
    <div class="detail-body">
      <p><strong>Kolmogorov Complexity \(K(x)\):</strong> \(x\)를 출력하는 가장 짧은 (prefix-free) 프로그램의 길이.</p>
      <p>시퀀스 \(x\)가 c-random이면 \(K(x) > |x| - c\). 즉, 거의 압축 불가능. 랜덤 시퀀스는 <strong>비계산적(incomputable)</strong>이다 — 어떤 프로그램도 이를 생성할 수 없다.</p>
      <div class="quote" style="margin-top: 0.8rem;">
        "Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin."
        <div class="quote-author">— John von Neumann, 1951</div>
      </div>
      <p>하지만 이 관점은 CSPRNG의 실용적 성공을 설명하지 못한다!</p>
    </div>
  </details>
  <details class="reveal reveal-delay-3" open>
    <summary>Level 3: 암호학적 랜덤 — 다항 시간 알고리즘이 예측 불가</summary>
    <div class="detail-body">
      <p><strong>CSPRNG (Def 3):</strong> 함수 \(G: \{0,1\}^k \to \{0,1\}^n\) (\(n \gg k\))의 출력이 다항 시간 구별기 \(D\)에 대해:</p>
      <div class="equation-block">$$\left| \Pr_{s \sim U_k}[D(G(s)) = 1] - \Pr_{u \sim U_n}[D(u) = 1] \right| < \text{negl}(k)$$</div>
      <p>짧은 시드(\(k\) bits)로 결정론적으로 긴 시퀀스(\(n\) bits)를 생성하지만, 다항 시간 관찰자에게는 <strong>진짜 랜덤과 구별 불가</strong>.</p>
      <p>예: JAX의 랜덤 생성 — \(u_k = E(k, s)\)에서 E는 Threefish block cipher, s는 시드.</p>
    </div>
  </details>

  <table class="data-table reveal reveal-delay-4">
    <tr><th>관찰자</th><th>계산 능력</th><th>CSPRNG 출력 판정</th><th>K(x) ≈</th></tr>
    <tr><td>무한 계산</td><td>제한 없음</td><td>비랜덤 (시드 역추적 가능)</td><td>\(k + O(1)\)</td></tr>
    <tr><td>다항 시간</td><td>poly(n)</td><td><span class="hl hl-pink">랜덤 (구별 불가)</span></td><td>해당 없음</td></tr>
    <tr><td>신경망</td><td>고정 FLOP</td><td>더 많은 것이 랜덤으로 보임</td><td>해당 없음</td></tr>
  </table>

  <div class="callout callout-gray reveal">
    <div class="callout-icon">&#128273;</div>
    <div><strong>핵심 통찰:</strong> "랜덤"은 객체의 본질적 속성이 아니라 <span class="hl hl-blue">관찰자의 계산 능력에 의존하는 상대적 개념</span>이다. 이 관점이 epiplexity의 출발점이다.</div>
  </div>
</section>


<!-- ══════════ 05. SOPHISTICATION ══════════ -->
<section class="section" id="sec05">
  <div class="reveal">
    <div class="section-number">05 — Sophistication</div>
    <span class="section-tag tag-background">Background</span>
    <h2 class="section-title">기존의 구조적 정보 측정: Sophistication</h2>
    <p class="section-subtitle">무한 계산을 가정한 기존 정의가 왜 실패하는가?</p>
  </div>

  <div class="two-col reveal reveal-delay-1">
    <div>
      <h3 class="block-title" style="margin-top: 0;">Naive Sophistication (Def 5)</h3>
      <p>문자열 \(x\)의 구조적 정보를 측정하려는 시도. \(x\)가 "랜덤 원소"인 집합 \(S\) 중 <strong>가장 짧은 기술의 집합</strong>의 복잡도:</p>
      <div class="equation-block">$$\text{nsoph}_c(x) = \min_S \{ K(S) : K(x|S) > \log|S| - c \}$$</div>
      <p><strong>직관:</strong> 구조적 부분(\(K(S)\))과 랜덤 부분(\(K(x|S)\))을 분리.</p>
    </div>
    <div>
      <h3 class="block-title" style="margin-top: 0;">Sophistication의 근본적 한계</h3>
      <ul class="clean-list accent-pink">
        <li><strong>Chaitin의 불완전성:</strong> 특정 문자열이 높은 sophistication을 갖는다는 것을 <em>증명할 수 없다</em></li>
        <li><strong>계산 무제한:</strong> 유체 혼합의 복잡한 행동도 무한 계산 + 초기 조건이면 단순 프로그램으로 재현 가능 → 낮은 sophistication</li>
        <li><strong>CSPRNG 오판:</strong> CSPRNG 출력은 짧은 프로그램(\(G + \text{seed}\))으로 생성 가능 → 낮은 Kolmogorov complexity. 하지만 다항 시간 관찰자에게는 <strong>랜덤</strong>이어야 함</li>
        <li><strong>실용 불가:</strong> 높은 sophistication의 최적 프로그램 실행 시간이 계산 가능한 함수보다 빠르게 증가</li>
      </ul>
    </div>
  </div>

  <div class="callout callout-orange reveal reveal-delay-2">
    <div class="callout-icon">&#9888;</div>
    <div><strong>왜 time-bounded Kolmogorov complexity로 대체하면 안 되나?</strong> CSPRNG 출력은 짧고 효율적으로 실행 가능한 생성 프로그램이 있어서 시간 제한 K도 작다. Sophistication에 시간 제한을 넣으면 <strong>모든 문자열에 대해 trivial</strong>해진다 (Appendix A.6). 근본적으로 다른 접근이 필요하다.</div>
  </div>

  <div class="callout callout-blue reveal reveal-delay-3">
    <div class="callout-icon">&#128218;</div>
    <div><strong>MDL Principle (Def 6):</strong> 모델 \(H\)와 모델에 의한 데이터 코딩의 <strong>총 기술 길이를 최소화</strong>: \(L(x) = \min_H [L(H) - \log P(x|H)]\). Sophistication이 "구조 분리"라면, MDL은 이를 실용적으로 구현한다. Epiplexity는 이 MDL에 <strong>계산 시간 제약</strong>을 추가한 것이다.</div>
  </div>
</section>


<!-- ══════════ 06. DEF: TIME-BOUNDED MODEL ══════════ -->
<section class="section" id="sec06">
  <div class="reveal">
    <div class="section-number">06 — Definition</div>
    <span class="section-tag tag-core">Core Definition</span>
    <h2 class="section-title">Time-bounded Probabilistic Model</h2>
    <p class="section-subtitle">Definition 7 — 계산 시간 \(T\) 이내에 샘플링과 확률 평가가 모두 가능한 확률 모델</p>
  </div>

  <div class="callout callout-blue reveal reveal-delay-1">
    <div class="callout-icon">&#128736;</div>
    <div>
      <strong>Definition 7:</strong> 프로그램 \(\mathsf{P}\)가 \(T\)-time probabilistic model이려면 두 가지를 시간 \(T(n)\) 이내에 수행해야 한다:
      <ul class="clean-list accent-blue" style="margin-top: 0.5rem;">
        <li><strong>Evaluation:</strong> 입력 \((0, x)\)에 대해 \(\text{Prob}_\mathsf{P}(x) \in [0,1]\)을 출력</li>
        <li><strong>Sampling:</strong> 입력 \((1, u)\) (무한 랜덤 테이프)에 대해 \(\text{Sample}_\mathsf{P}(u) \in \{0,1\}^n\)을 출력</li>
      </ul>
      두 출력은 일관적이어야 한다: \(\Pr_u[\text{Sample}(u) = x] = \text{Prob}(x)\) 이고 \(\sum_x \text{Prob}(x) = 1\).
    </div>
  </div>

  <p class="reveal reveal-delay-2"><strong>왜 두 조건 모두 필요한가?</strong> 대부분의 sequence model (autoregressive LM, diffusion model 등)은 샘플링과 확률 평가를 모두 지원한다. 이 정의는 계산 시간 \(T\) 이내에 <strong>함수 클래스의 "크기"</strong>를 제약한다.</p>

  <div class="callout callout-gray reveal reveal-delay-3">
    <div class="callout-icon">&#128221;</div>
    <div>
      <strong>표기법:</strong> \(\mathcal{P}_T\) = 모든 \(T\)-time probabilistic model의 집합. 이탤릭 \(P\)는 확률 질량 함수, 비이탤릭 \(\mathsf{P}\)는 프로그램 자체를 가리킨다. 시간 \(T\)는 주로 FLOP으로 측정한다.
    </div>
  </div>
</section>


<!-- ══════════ 07. DEF: EPIPLEXITY ══════════ -->
<section class="section" id="sec07">
  <div class="reveal">
    <div class="section-number">07 — Definition</div>
    <span class="section-tag tag-core">Core Definition</span>
    <h2 class="section-title">Epiplexity & Time-Bounded Entropy</h2>
    <p class="section-subtitle">Definition 8 — 계산 제약 하에서 MDL을 최소화하는 최적 프로그램으로 정보를 분해</p>
  </div>

  <div class="equation-block reveal reveal-delay-1">
    $$\mathsf{P}^* = \arg\min_{\mathsf{P} \in \mathcal{P}_T} \left\{ |\mathsf{P}| + \mathbb{E}\left[\log \frac{1}{P(X)}\right] \right\}$$
    <div style="margin-top: 1rem; font-size: 0.88rem; color: var(--text-secondary);">
      <span class="hl hl-blue">$S_T(X) = |\mathsf{P}^*|$</span> &nbsp;(Epiplexity: 최적 모델의 프로그램 크기) &nbsp;&nbsp;&nbsp;
      <span class="hl hl-pink">$H_T(X) = \mathbb{E}\!\left[\log \frac{1}{P^*(X)}\right]$</span> &nbsp;(Time-bounded Entropy: 잔여 랜덤)
    </div>
  </div>

  <div class="two-col reveal reveal-delay-2">
    <div class="card" style="--card-accent: var(--accent-blue); border-left: 3px solid var(--accent-blue); border-top: none;">
      <h4 class="hl-blue">Epiplexity \(S_T(X)\)</h4>
      <ul class="clean-list accent-blue">
        <li>데이터의 패턴을 설명하는 <strong>최적 모델의 프로그램 크기</strong> (bits)</li>
        <li>모델이 데이터에서 "배운 것"의 양</li>
        <li>재사용 가능한 회로/서브프로그램의 총 기술 길이</li>
        <li>계산 예산 \(T\) 증가 → 일반적으로 증가 (더 정교한 패턴 포착)</li>
        <li>데이터 크기 \(D\) 증가 → 일반적으로 증가</li>
      </ul>
    </div>
    <div class="card" style="--card-accent: var(--accent-pink); border-left: 3px solid var(--accent-pink); border-top: none;">
      <h4 class="hl-pink">Time-bounded Entropy \(H_T(X)\)</h4>
      <ul class="clean-list accent-pink">
        <li>최적 모델로도 남는 <strong>예측 불가능한 노이즈</strong></li>
        <li>In-distribution에서의 비예측성 (test loss에 대응)</li>
        <li>CSPRNG 출력은 거의 최대 (\(\approx n\))</li>
        <li>계산 예산 \(T\) 증가 → 일반적으로 감소</li>
        <li>총 정보 = \(\text{MDL}_T(X) = S_T(X) + H_T(X)\)</li>
      </ul>
    </div>
  </div>

  <div class="callout callout-yellow reveal reveal-delay-3">
    <div class="callout-icon">&#128218;</div>
    <div><strong>비유:</strong> 데이터가 퍼즐이라면, <span class="hl hl-blue">Epiplexity</span> = 퍼즐을 풀기 위한 전략/규칙의 복잡도, <span class="hl hl-pink">Time-bounded Entropy</span> = 전략을 다 써도 남는 운(luck)에 의존하는 부분. Uniform random은 전략이 필요 없고(\(S_T \approx 0\)), 모두 운이다(\(H_T \approx n\)). 복잡한 알고리즘 코드는 전략이 많고(\(S_T\) 큼), 운이 적다(\(H_T\) 작음).</div>
  </div>

  <h3 class="block-title reveal">조건부 Epiplexity (Def 11)</h3>
  <p class="reveal">이미지 분류처럼 입력 \(X\)가 주어졌을 때 \(Y\)의 구조만 관심 있는 경우:</p>
  <div class="equation-block reveal">
    $$\mathsf{P}^*_{Y|X} = \arg\min_{\mathsf{P} \in \mathcal{P}_T} \left\{ |\mathsf{P}| + \mathbb{E}_{(X,Y)}[-\log P(Y|X)] \right\}$$
  </div>
  <p class="reveal">ML 세팅에서 \(X\)는 전체 데이터셋 \([X_1, X_2, \ldots]\)을 의미하며, epiplexity는 데이터셋 크기에 따라 증가하는 것이 일반적이다.</p>
</section>


<!-- ══════════ 08. PROPERTIES ══════════ -->
<section class="section" id="sec08">
  <div class="reveal">
    <div class="section-number">08 — Properties & Theorems</div>
    <span class="section-tag tag-core">Core</span>
    <h2 class="section-title">기본 성질 & 핵심 정리</h2>
    <p class="section-subtitle">Epiplexity와 Time-bounded Entropy의 핵심 수학적 성질들</p>
  </div>

  <h3 class="block-title reveal">기본 성질 (Basic Properties)</h3>
  <div class="prop-grid reveal reveal-delay-1">
    <div class="prop-item">
      <h4>(1) 비음수성</h4>
      <p>\(S_T(X) \geq 0\), \(H_T(X) \geq 0\)</p>
    </div>
    <div class="prop-item">
      <h4>(2) 총 정보 범위</h4>
      <p>\(H(X) \leq S_T + H_T \leq n + c\)</p>
    </div>
    <div class="prop-item">
      <h4>(3) 계산 ↑ → MDL ↓</h4>
      <p>\(T' \geq T \Rightarrow \text{MDL}_{T'} \leq \text{MDL}_T\)</p>
    </div>
    <div class="prop-item" style="border-color: var(--accent-pink);">
      <h4>(4) 비대칭 변환 (핵심!)</h4>
      <p>\(\text{MDL}_{T'}(f^{-1}(X)) \leq \text{MDL}_T(X) + |f| + c\), 단 \(T' = T + \text{Time}(f)\). <strong>\(f\)와 \(f^{-1}\)의 비대칭성이 역설 해소의 핵심!</strong></p>
    </div>
  </div>
  <p class="reveal reveal-delay-2" style="font-size: 0.86rem; color: var(--text-secondary);">성질 (4)에서 핵심: Kolmogorov에서는 \(K(f) = K(f^{-1}) + O(1)\)이지만, 시간 제한이 있으면 \(f\)의 짧은 프로그램이 있다고 \(f^{-1}\)의 짧은 프로그램이 보장되지 않는다. 이 비대칭이 세 역설을 관통하는 핵심 메커니즘이다.</p>

  <h3 class="block-title reveal">Theorem 9: CSPRNG은 최대 랜덤, 최소 구조</h3>
  <div class="callout callout-blue reveal reveal-delay-1">
    <div class="callout-icon">&#128274;</div>
    <div>
      <strong>Thm 9:</strong> CSPRNG \(G\)가 \(k\) bit → \(n\) bit 확장, advantage \(\varepsilon(k)\)이면:
      <ul class="clean-list accent-blue" style="margin-top: 0.5rem;">
        <li>Time-bounded entropy: \(n - 2 - n\varepsilon(k) < H_{\text{Poly}}(G(U_k)) \leq n + c\) (거의 최대!)</li>
        <li>Epiplexity: \(S_{\text{Poly}}(G(U_k)) \leq c + n\varepsilon(k)\) (거의 0!)</li>
      </ul>
      <p style="margin-top: 0.5rem; margin-bottom: 0;">반면 Shannon entropy는 \(H(G(U_k)) = k\), Kolmogorov도 \(\leq k + c\). Epiplexity만이 CSPRNG 출력을 "높은 랜덤, 낮은 구조"로 올바르게 특성화한다.</p>
    </div>
  </div>

  <h3 class="block-title reveal">Theorem 10: 높은 Epiplexity가 존재한다</h3>
  <div class="callout callout-purple reveal reveal-delay-1">
    <div class="callout-icon">&#128200;</div>
    <div>
      <strong>Thm 10:</strong> One-way function이 존재하면, \(S_{\text{Poly}}(X_n) = \Omega(\log n)\)인 확률 변수 시퀀스 \(\{X_n\}\)이 존재한다.
      <p style="margin-top: 0.5rem; margin-bottom: 0;">이론적 하한은 \(\Omega(\log n)\)으로 겸손하지만, 실제 자연 데이터에서는 power law 스케일링이 관찰된다. 이론과 실제의 갭은 향후 연구 과제이다.</p>
    </div>
  </div>
</section>


<!-- ══════════ 09. PREQUENTIAL CODING ══════════ -->
<section class="section" id="sec09">
  <div class="reveal">
    <div class="section-number">09 — Measurement</div>
    <span class="section-tag tag-experiment">Measurement</span>
    <h2 class="section-title">측정 방법 1: Prequential Coding</h2>
    <p class="section-subtitle">Section 4.1 — Loss 곡선의 "최종 loss 위 면적"으로 Epiplexity를 추정</p>
  </div>

  <div class="step-flow reveal reveal-delay-1">
    <div class="step-item">
      <div class="step-num">1</div>
      <h4>랜덤 초기화</h4>
      <p>모델 \(P_0\) 시작</p>
    </div>
    <div class="step-arrow">&#8594;</div>
    <div class="step-item">
      <div class="step-num">2</div>
      <h4>순차 코딩</h4>
      <p>\(Z_i\)를 \(\log 1/P_i(Z_i)\) bits로 코딩 후 학습</p>
    </div>
    <div class="step-arrow">&#8594;</div>
    <div class="step-item">
      <div class="step-num">3</div>
      <h4>면적 계산</h4>
      <p>Loss 곡선 - 최종 loss = 모델 기술 길이</p>
    </div>
  </div>

  <div class="equation-block reveal reveal-delay-2">
    $$|\mathsf{P}_{\text{preq}}| \approx \sum_{i=0}^{M-1} \left( \log \frac{1}{P_i(Z_i)} - \log \frac{1}{P_M(Z_i)} \right)$$
    <div style="margin-top: 0.8rem; font-size: 0.85rem; color: var(--text-secondary);">= Loss 곡선에서 최종 loss 위의 면적 (i.i.d. 샘플 가정)</div>
  </div>

  <div class="two-col reveal reveal-delay-3">
    <div>
      <h3 class="block-title" style="margin-top: 0;">직관적 이해</h3>
      <ul class="clean-list accent-blue">
        <li><strong>랜덤 데이터:</strong> loss가 전혀 감소하지 않음 → 면적 = 0 → \(S_T \approx 0\)</li>
        <li><strong>단순 데이터:</strong> loss가 빠르게 감소하고 안정화 → 면적 작음 → \(S_T\) 작음</li>
        <li><strong>복잡한 구조적 데이터:</strong> loss가 오랜 시간에 걸쳐 꾸준히 감소 → 면적 큼 → <strong>\(S_T\) 큼</strong></li>
      </ul>
    </div>
    <div>
      <h3 class="block-title" style="margin-top: 0;">장단점</h3>
      <ul class="clean-list accent-green">
        <li>기존 학습 loss 곡선만 있으면 즉시 추정 가능</li>
        <li>개념적으로 단순하고 직관적</li>
      </ul>
      <ul class="clean-list accent-pink" style="margin-top: 0.5rem;">
        <li>엄밀한 상한이 아님 (symmetry of information 근사에 의존)</li>
        <li>런타임 보장이 없음</li>
      </ul>
    </div>
  </div>
</section>


<!-- ══════════ 10. REQUENTIAL CODING ══════════ -->
<section class="section" id="sec10">
  <div class="reveal">
    <div class="section-number">10 — Measurement</div>
    <span class="section-tag tag-experiment">Measurement</span>
    <h2 class="section-title">측정 방법 2: Requential Coding</h2>
    <p class="section-subtitle">Section 4.2 — Teacher-Student KL divergence로 모델의 명시적 코드 구성</p>
  </div>

  <p class="reveal reveal-delay-1">Prequential coding의 한계를 해결하기 위해, <strong>모델 자체의 명시적 코드</strong>를 구성한다. Teacher 모델이 Student 모델에게 합성 데이터를 생성해주며, Student가 Teacher를 따라잡는 과정 전체가 코드가 된다.</p>

  <div class="step-flow reveal reveal-delay-2">
    <div class="step-item">
      <div class="step-num" style="background: var(--accent-green);">1</div>
      <h4>Teacher 학습</h4>
      <p>실 데이터로 Teacher \(P^t_i\) 체크포인트 저장</p>
    </div>
    <div class="step-arrow">&#8594;</div>
    <div class="step-item">
      <div class="step-num" style="background: var(--accent-green);">2</div>
      <h4>합성 토큰 생성</h4>
      <p>\(\tilde{Z}_i \sim P^t_i\)를 상대 엔트로피 코딩</p>
    </div>
    <div class="step-arrow">&#8594;</div>
    <div class="step-item">
      <div class="step-num" style="background: var(--accent-green);">3</div>
      <h4>KL 합산</h4>
      <p>\(|\mathsf{P}_{\text{req}}| = \sum \text{KL}(P^t_i \| P^s_i)\)</p>
    </div>
  </div>

  <div class="equation-block reveal reveal-delay-3">
    $$|\mathsf{P}_{\text{req}}| = \sum_{i=0}^{M-1} \text{KL}(P^t_i \| P^s_i) + \log(1 + \text{KL}(P^t_i \| P^s_i)) + 4 + O(1) \approx \sum_{i=0}^{M-1} \text{KL}(P^t_i \| P^s_i)$$
    <div style="margin-top: 0.8rem; font-size: 0.85rem; color: var(--text-secondary);">= Teacher와 Student loss 곡선 사이의 면적 (시각적 근사)</div>
  </div>

  <div class="callout callout-green reveal">
    <div class="callout-icon">&#10003;</div>
    <div>
      <strong>Requential의 장점:</strong> (1) <strong>명시적 코드</strong>를 제공 — 모델 크기의 상한이 보장됨. (2) 런타임도 보장됨 (\(6ND + 2ND\)). (3) Teacher 선택의 유연성.
      <br><strong>단점:</strong> 2~10배 느림 (반복적인 Teacher 샘플링 필요).
    </div>
  </div>

  <div class="figure reveal reveal-delay-1">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig2_v2.png" alt="Figure 2: How to estimate epiplexity">
    </div>
    <div class="figure-caption"><strong>Figure 2.</strong> (a) Prequential: loss 곡선-최종 loss 면적. Requential: Teacher-Student loss 갭 면적. (b) Compute 예산별 최적 2-part 코드. (c) 두 방법의 상관관계.</div>
  </div>
</section>

<!-- ══════════ 11. COMPARISON & SCALING ══════════ -->
<section class="section" id="sec11">
  <div class="reveal">
    <div class="section-number">11 — Comparison</div>
    <span class="section-tag tag-experiment">Analysis</span>
    <h2 class="section-title">두 방법 비교 & 스케일링 행동</h2>
    <p class="section-subtitle">Section 4.3-4.4 — Prequential vs Requential, 그리고 Compute/Data에 따른 변화</p>
  </div>

  <div class="vs-box reveal reveal-delay-1">
    <div class="vs-side" style="border-top: 3px solid var(--accent-blue);">
      <h4 style="color: var(--accent-blue); margin-bottom: 6px;">Prequential</h4>
      <p style="font-size: 0.85rem; color: var(--text-secondary); line-height: 1.7;">간편, 기존 loss 곡선 활용<br>Heuristic (symmetry of information)<br>일반적으로 <strong>더 큰</strong> 추정값</p>
    </div>
    <div class="vs-divider">vs</div>
    <div class="vs-side" style="border-top: 3px solid var(--accent-green);">
      <h4 style="color: var(--accent-green); margin-bottom: 6px;">Requential</h4>
      <p style="font-size: 0.85rem; color: var(--text-secondary); line-height: 1.7;">엄밀, 명시적 코드 제공<br>런타임 보장<br>2~10배 느림, <strong>더 정확</strong></p>
    </div>
  </div>

  <p class="reveal reveal-delay-2">Figure 2c에서 보듯, 두 방법은 <strong>잘 상관</strong>되며, 특히 같은 데이터 그룹 내에서는 순위가 일치한다. Prequential 추정값이 일반적으로 수 배 더 크지만, 데이터셋 간 비교에는 충분히 유용하다.</p>

  <h3 class="block-title reveal">Compute & Data에 따른 스케일링</h3>
  <div class="prop-grid reveal reveal-delay-1">
    <div class="prop-item">
      <h4>Compute \(T\) 증가 시</h4>
      <p>최적 모델 크기 \(N^*(T)\) ↑, 학습 데이터 \(D^*(T)\) ↑ → <span class="hl hl-blue">\(S_T\) 증가</span>, <span class="hl hl-pink">\(H_T\) 감소</span></p>
    </div>
    <div class="prop-item">
      <h4>Data \(D\) 증가 시</h4>
      <p>무한 compute에서 \(S_\infty(X)\)는 \(D\)와 함께 증가. 토큰당 \(H_\infty / D\)는 감소.</p>
    </div>
    <div class="prop-item">
      <h4>직관</h4>
      <p>더 많은 계산 + 더 많은 데이터 → 더 정교한 패턴 추출 → 남은 랜덤 감소</p>
    </div>
  </div>
</section>


<!-- ══════════ 12. PARADOX 1 THEORY ══════════ -->
<section class="section" id="sec12">
  <div class="reveal">
    <div class="section-number">12 — Paradox 1: Theory</div>
    <span class="section-tag tag-paradox">Paradox 1</span>
    <h2 class="section-title">결정론적 변환으로 정보를 만들 수 있다</h2>
    <p class="section-subtitle">Section 5.1 — CSPRNG이 time-bounded 정보를 극적으로 증가시킨다</p>
  </div>

  <div class="callout callout-pink reveal reveal-delay-1" style="font-size: 0.85rem;">
    <div class="callout-icon">&#128683;</div>
    <div><strong>기존 이론:</strong> Data Processing Inequality — \(H(f(X)) \leq H(X)\). Kolmogorov — \(K(f(x)) \leq K(x) + K(f) + c\). 결정론적 함수는 정보를 <em>절대</em> 증가시킬 수 없다.</div>
  </div>

  <h3 class="block-title reveal">Theorem 12: CSPRNG의 정보 증폭</h3>
  <div class="equation-block reveal reveal-delay-1">
    $$H_{\text{Poly}}(G(U_k)) > H_{\text{Poly}}(U_k) + n - n\varepsilon(k) - k - O(1)$$
  </div>

  <div class="two-col reveal reveal-delay-2">
    <div>
      <p>\(k\) bits 시드 → \(n\) bits 출력 (\(n \gg k\)).</p>
      <ul class="clean-list accent-blue">
        <li>입력: \(H_{\text{Poly}}(U_k) = k\) (진짜 랜덤 \(k\) bits)</li>
        <li>출력: \(H_{\text{Poly}}(G(U_k)) \approx n\) (다항 시간 관찰자에게 \(n\) bits 랜덤)</li>
        <li>증가분: <strong>\(\approx n - k\) bits</strong>의 time-bounded 정보가 "생성"됨</li>
      </ul>
      <p style="margin-top: 0.5rem;">결정론적 함수 \(G\)가 time-bounded 정보를 증가시킨 것이다! 핵심은 <span class="hl hl-pink">\(G\)의 역함수가 효율적으로 계산 불가</span>하다는 것.</p>
    </div>
    <div>
      <div class="callout callout-gray">
        <div class="callout-icon">&#128161;</div>
        <div>
          <strong>직관:</strong> 무한 계산 관찰자는 \(G^{-1}\)을 실행하여 시드를 복원 → 정보량 = \(k\). 다항 시간 관찰자는 \(G^{-1}\)을 실행할 수 없음 → 출력이 랜덤으로 보임 → 정보량 = \(n\).
          <br><br><strong>합성 데이터 시사점:</strong> 역함수가 효율적으로 계산 불가능한 변환을 사용하면, 합성 데이터도 새로운 정보를 "생성"할 수 있다.
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ══════════ 13. PARADOX 1 EXPERIMENT ══════════ -->
<section class="section" id="sec13">
  <div class="reveal">
    <div class="section-number">13 — Paradox 1: Experiment</div>
    <span class="section-tag tag-experiment">Experiment</span>
    <h2 class="section-title">Elementary Cellular Automata 실험</h2>
    <p class="section-subtitle">같은 형식의 간단한 규칙에서 근본적으로 다른 정보가 생성된다</p>
  </div>

  <p class="reveal reveal-delay-1">Elementary Cellular Automata (ECA)는 1차원 이진 셀 배열이 간단한 규칙으로 진화하는 시스템이다 (256가지 규칙). 랜덤 초기 조건 \(X\)에서 48 단계 진화시킨 결과 \(Y = F(X)\)를 예측하는 LLM을 학습한다.</p>

  <div class="card-grid reveal reveal-delay-2" style="grid-template-columns: repeat(3, 1fr);">
    <div class="card" style="--card-accent: var(--accent-green);">
      <h4><span class="hl hl-green">Rule 15</span> (Class II)</h4>
      <p>주기적 패턴. 역함수 간단.<br>Loss 빠르게 수렴.<br><strong>\(H_T\) 낮음, \(S_T\) 낮음</strong></p>
    </div>
    <div class="card" style="--card-accent: var(--accent-red);">
      <h4><span class="hl hl-red">Rule 30</span> (Class III)</h4>
      <p>카오스적. 역함수 어려움 (추정).<br>Loss 거의 불변.<br><strong>\(H_T\) 최대, \(S_T\) 낮음</strong></p>
    </div>
    <div class="card" style="--card-accent: var(--accent-purple);">
      <h4><span class="hl hl-purple">Rule 54</span> (Class IV)</h4>
      <p>복잡하지만 부분 예측 가능.<br>Loss 꾸준히 감소.<br><strong>\(H_T\) 중간, \(S_T\) 높음!</strong></p>
    </div>
  </div>

  <div class="figure reveal reveal-delay-3">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig3_v2.png" alt="Figure 3: ECA experiments">
    </div>
    <div class="figure-caption"><strong>Figure 3.</strong> 왼쪽: ECA 시각화 (시간은 위→아래). Rule 15는 단순 주기, Rule 30은 카오스, Rule 54는 글라이더 등 복잡한 구조. 오른쪽: compute에 따른 MDL, \(H_T\), \(S_T\) 변화. Rule 54만이 유의미한 epiplexity를 생산한다.</div>
  </div>

  <div class="callout callout-green reveal">
    <div class="callout-icon">&#128161;</div>
    <div><strong>핵심:</strong> 거의 동일한 프로그램(3가지 ECA 규칙)에서 <strong>근본적으로 다른 종류의 정보</strong>가 생성된다. Epiplexity는 이 차이를 정확히 포착한다. Rule 54처럼 "부분적으로 예측 가능한 복잡성"이 높은 epiplexity의 원천이다.</div>
  </div>

  <div class="figure reveal">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig1_v2.png" alt="Figure 1: Random vs Structural Information">
    </div>
    <div class="figure-caption"><strong>Figure 1.</strong> 왼쪽: 구조적/랜덤 정보 스펙트럼 — 반복 코드(낮음/낮음), 알고리즘(높음/중간), API 키(낮음/높음). 오른쪽 상단: 계산으로 정보 생성 (ECA, Lorenz attractor). 오른쪽 하단: Epiplexity → OOD 일반화 (재사용 가능한 회로).</div>
  </div>
</section>


<!-- ══════════ 14. PARADOX 2 THEORY ══════════ -->
<section class="section" id="sec14">
  <div class="reveal">
    <div class="section-number">14 — Paradox 2: Theory</div>
    <span class="section-tag tag-paradox">Paradox 2</span>
    <h2 class="section-title">데이터 순서가 학습되는 정보를 바꾼다</h2>
    <p class="section-subtitle">Section 5.2 — 동일한 데이터도 factorization 순서에 따라 다른 time-bounded 정보를 가진다</p>
  </div>

  <div class="callout callout-pink reveal reveal-delay-1" style="font-size: 0.85rem;">
    <div class="callout-icon">&#128683;</div>
    <div><strong>기존 이론:</strong> Symmetry of Information — \(H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X)\). 순서가 바뀌어도 총 정보량은 동일. Kolmogorov에서도 \(K(Y|X) + K(X) = K(X|Y) + K(Y) + O(1)\).</div>
  </div>

  <h3 class="block-title reveal">Theorem 13: One-Way Permutation과 순서 비대칭</h3>
  <div class="equation-block reveal reveal-delay-1">
    $$H_{\text{Poly}}(X|Y) + H_{\text{Poly}}(Y) > H_{\text{Poly}}(Y|X) + H_{\text{Poly}}(X) + \omega(\log n)$$
    <div style="margin-top: 0.8rem; font-size: 0.86rem; color: var(--text-secondary);">
      여기서 \(X = U_n\), \(Y = f(X)\)이고 \(f\)는 one-way permutation.
    </div>
  </div>

  <div class="two-col reveal reveal-delay-2">
    <div>
      <h3 class="block-title" style="margin-top: 0;">직관적 이해</h3>
      <ul class="clean-list accent-purple">
        <li><strong>정방향:</strong> \(X\) → \(Y = f(X)\). \(X\)를 알면 \(f\)를 실행하여 \(Y\)를 쉽게 예측 → \(H_{\text{Poly}}(Y|X) \approx 0\)</li>
        <li><strong>역방향:</strong> \(Y\) → \(X = f^{-1}(Y)\). \(Y\)를 알아도 \(f^{-1}\)이 비효율적이면 \(X\)를 예측 불가 → \(H_{\text{Poly}}(X|Y) \approx n\)</li>
        <li>결과: 정방향과 역방향의 <strong>총 time-bounded 정보량이 다르다</strong></li>
      </ul>
    </div>
    <div>
      <div class="callout callout-blue">
        <div class="callout-icon">&#128274;</div>
        <div>
          <strong>따름정리 (Thm 26):</strong> 다항 시간 확률 모델은 one-way function의 정방향을 잘 맞추더라도, <strong>Bayes' theorem을 만족시킬 수 없다</strong>. 즉, \(P(X|Y)\)와 \(P(Y|X)\)가 동시에 정확할 수 없다.
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ══════════ 15. PARADOX 2 EXPERIMENT ══════════ -->
<section class="section" id="sec15">
  <div class="reveal">
    <div class="section-number">15 — Paradox 2: Experiment</div>
    <span class="section-tag tag-experiment">Experiment</span>
    <h2 class="section-title">체스 실험: Forward vs Reverse</h2>
    <p class="section-subtitle">동일한 체스 게임 데이터를 두 가지 순서로 모델링하면 결과가 달라진다</p>
  </div>

  <div class="vs-box reveal reveal-delay-1">
    <div class="vs-side">
      <h4 style="margin-bottom: 6px;">Forward</h4>
      <p style="font-size: 0.85rem; color: var(--text-secondary);">수순 → 최종 보드<br><span style="font-size: 0.8rem; color: var(--text-tertiary);">간단한 시뮬레이션으로 보드 상태 계산 가능</span></p>
    </div>
    <div class="vs-divider">vs</div>
    <div class="vs-side" style="border-top: 3px solid var(--accent-purple);">
      <h4 style="color: var(--accent-purple); margin-bottom: 6px;">Reverse</h4>
      <p style="font-size: 0.85rem; color: var(--text-secondary);">최종 보드 → 수순<br><span class="hl hl-purple" style="font-size: 0.8rem;">보드에서 수순 역추론 필요 → 더 복잡한 전략 학습</span></p>
    </div>
  </div>

  <p class="reveal reveal-delay-2">Lichess 데이터셋에서 각 게임을 (1) [수순, 최종 FEN] 순서와 (2) [최종 FEN, 수순] 순서로 각각 autoregressive transformer를 학습. Figure 4c에서:</p>

  <ul class="clean-list accent-purple reveal reveal-delay-2">
    <li><strong>Reverse 순서:</strong> 더 높은 \(H_T\) (예측하기 어려움) + 더 높은 \(S_T\) (더 많은 구조 학습)</li>
    <li>낮은 compute에서는 두 순서의 차이 미미 → 표면 통계만 학습</li>
    <li>compute 증가 시 갭이 확대 → Reverse가 더 깊은 보드 상태 표현을 학습</li>
  </ul>

  <div class="figure reveal reveal-delay-3">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig4_v2.png" alt="Figure 4: Factorization matters">
    </div>
    <div class="figure-caption"><strong>Figure 4.</strong> (a) ECA Rule 30으로 구현한 OWF: 정방향은 Shannon entropy에 도달하지만, 역방향은 영구적 갭 존재. (b) 체스 두 순서의 직관적 비교. (c) Reverse 순서가 더 높은 \(H_T\)와 \(S_T\)를 가진다.</div>
  </div>
</section>


<!-- ══════════ 16. PARADOX 3A: INDUCTION ══════════ -->
<section class="section" id="sec16">
  <div class="reveal">
    <div class="section-number">16 — Paradox 3A: Induction</div>
    <span class="section-tag tag-paradox">Paradox 3</span>
    <h2 class="section-title">모델은 생성 과정보다 더 많이 배울 수 있다: 귀납</h2>
    <p class="section-subtitle">Section 5.3.1 — 예측 모델이 데이터 생성자보다 복잡한 전략을 학습한다</p>
  </div>

  <div class="quote reveal reveal-delay-1">
    "You're reading a murder mystery and at some point the text reveals the identity of the criminal. ... If the model can predict [the name] then it must have figured out [who perpetrated the murder from the evidence provided]."
    <div class="quote-author">— Ilya Sutskever, 2019</div>
  </div>

  <p class="reveal reveal-delay-2">소설의 <strong>저자</strong>는 범인을 먼저 정하고 이야기를 쓴다 (간단한 생성 과정). 하지만 <strong>예측 모델</strong>은 증거에서 범인을 <em>귀납적으로 추론</em>해야 한다 — 데이터 생성 과정에는 없던 복잡한 전략이 필요하다.</p>

  <h3 class="block-title reveal">실험 설계: 숨겨진 정보 + 변환</h3>
  <p class="reveal">랜덤 변수 \(Z\)에서 마스킹 \(m\)으로 \(h\) bits를 숨기고, 변환 \(f\)를 적용: \(Y = (m(Z), f(Z))\). 예측 모델은 숨겨진 \(h\) bits에 대해 <strong>귀납</strong>해야 한다.</p>

  <div class="two-col reveal reveal-delay-1">
    <div>
      <h3 class="block-title" style="margin-top: 0;"><span class="hl hl-red">Hard Induction</span>: Rule 30 ECA</h3>
      <ul class="clean-list accent-pink">
        <li>\(f\) = Rule 30 ECA 4단계, \(h\) bits 숨김</li>
        <li>모델은 \(2^h\)개 후보를 검토해야 함</li>
        <li>수렴에 필요한 compute가 \(h\)에 <strong>지수적</strong>으로 증가</li>
        <li>\(h > 0\)이면 epiplexity 증가</li>
      </ul>
    </div>
    <div>
      <h3 class="block-title" style="margin-top: 0;"><span class="hl hl-green">Easy Induction</span>: Random Markov Chain</h3>
      <ul class="clean-list accent-green">
        <li>\(f\) = Markov chain 샘플, \(h\) rows 숨김</li>
        <li>통계적 귀납(in-context learning)으로 해결 가능</li>
        <li>\(0 < h < 8\)일 때 <strong>epiplexity 최대</strong></li>
        <li>제공된 정보 + 귀납 전략 = 두 전략을 동시 학습</li>
      </ul>
    </div>
  </div>

  <div class="figure reveal reveal-delay-2">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig5_v2.png" alt="Figure 5: Induction experiments">
    </div>
    <div class="figure-caption"><strong>Figure 5.</strong> (a) 데이터 생성: Z를 마스킹(m) + 변환(f)하여 Y 생성. (b) Hard: h가 클수록 지수적 compute 필요. (c) Easy: 0 &lt; h &lt; 8일 때 두 전략이 결합되어 epiplexity 최대.</div>
  </div>

  <div class="callout callout-orange reveal">
    <div class="callout-icon">&#128161;</div>
    <div><strong>시사점:</strong> 귀납 전략은 데이터 생성 과정에 없던 것이지만, MLE 모델링을 통해 자연스럽게 학습된다. 이는 autoregressive 모델에 국한되지 않으며, VAE의 encoder가 posterior \(P(Z|X)\)를 학습하는 것도 같은 현상이다 (Appendix G).</div>
  </div>
</section>


<!-- ══════════ 17. PARADOX 3B: EMERGENCE ══════════ -->
<section class="section" id="sec17">
  <div class="reveal">
    <div class="section-number">17 — Paradox 3B: Emergence</div>
    <span class="section-tag tag-paradox">Paradox 3</span>
    <h2 class="section-title">모델은 생성 과정보다 더 많이 배울 수 있다: 창발</h2>
    <p class="section-subtitle">Section 5.3.2 — 간단한 규칙의 반복이 계산 제한 관찰자에게 새로운 구조를 드러낸다</p>
  </div>

  <p class="reveal reveal-delay-1">Conway's Game of Life나 ECA Rule 54처럼 간단한 규칙 \(\Phi\)를 \(k\)번 반복 적용하면(\(\Phi^k\)), <span class="hl hl-purple">글라이더, 진동자, 충돌 규칙</span> 등 <strong>창발적 패턴</strong>이 나타난다.</p>

  <div class="two-col reveal reveal-delay-2">
    <div>
      <h3 class="block-title" style="margin-top: 0;">두 관찰자의 차이</h3>
      <ul class="clean-list">
        <li><strong>충분한 계산 (\(T_2\)):</strong> \(\Phi\)를 \(k\)번 직접 반복 실행 → 짧은 프로그램으로 충분 → \(S_{T_2}\) 작음</li>
        <li><strong>제한된 계산 (\(T_1\)):</strong> \(\Phi^k\)를 직접 반복할 여유 없음 → 글라이더 종류, 속도, 충돌 규칙 등을 학습 → <strong>\(S_{T_1}\) 큼</strong></li>
      </ul>

      <div class="callout callout-purple" style="margin-top: 1rem;">
        <div class="callout-icon">&#128300;</div>
        <div>
          <strong>Definition 14 (Epiplexity Emergent):</strong> 한 단계(\(\Phi\))에서 두 관찰자의 epiplexity 차이는 상수이지만, 다단계(\(\Phi^k\))에서 차이가 무한히 커지면 "epiplexity emergent"이다.
        </div>
      </div>
    </div>
    <div>
      <div class="figure" style="margin-top: 0;">
        <div class="figure-frame" onclick="openLightbox(this)">
          <img src="img/fig6_v2.png" alt="Figure 6: Emergence in ECA">
        </div>
        <div class="figure-caption"><strong>Figure 6.</strong> Non-looped 모델: 직접 최종 상태 예측 → compute 증가에 따라 epiplexity 증가 (점점 더 많은 종을 학습). Looped 모델: 충분한 compute에서 brute-force(중간 단계 반복) 학습 → epiplexity 급락.</div>
      </div>
    </div>
  </div>

  <div class="callout callout-key reveal">
    <div class="callout-icon">&#128161;</div>
    <div>
      <strong>Chain-of-thought 연결:</strong> Looped transformer가 중간 단계를 반복하는 것은 <strong>chain-of-thought</strong>와 유사하다. 충분한 compute가 있으면 brute-force가 가능해지며, 이때 epiplexity가 오히려 <em>감소</em>한다. 실제 대부분의 자연 데이터에서는 brute-force가 비현실적이므로, 더 많은 compute = 더 높은 epiplexity.
    </div>
  </div>
</section>


<!-- ══════════ 18. OOD GENERALIZATION ══════════ -->
<section class="section" id="sec18">
  <div class="reveal">
    <div class="section-number">18 — Application</div>
    <span class="section-tag tag-application">Application</span>
    <h2 class="section-title">Epiplexity와 OOD 일반화</h2>
    <p class="section-subtitle">Section 6.1 — 더 높은 Epiplexity ≈ 더 좋은 OOD 성능의 신호</p>
  </div>

  <p class="reveal reveal-delay-1">OOD 일반화는 모델이 <strong>재사용 가능한 구조</strong>를 얼마나 학습했느냐에 달려 있다. 같은 loss를 달성하더라도 학습한 구조의 양(epiplexity)이 다르면 OOD 성능이 달라진다.</p>

  <h3 class="block-title reveal">체스 OOD 실험</h3>
  <div class="two-col reveal reveal-delay-1">
    <div>
      <ul class="clean-list accent-blue">
        <li>Forward / Reverse 순서로 사전학습한 모델을 두 가지 downstream task에 fine-tune</li>
        <li><strong>Task 1: 체스 퍼즐</strong> — 보드 상태에서 최적 수 예측</li>
        <li><strong>Task 2: Centipawn 평가</strong> — FEN 표기에서 포지션 이점 평가 (더 큰 distribution shift)</li>
        <li>Reverse 순서(높은 \(S_T\)): 퍼즐 정확도 유사, <strong>Centipawn에서 유의미한 우위</strong></li>
      </ul>
    </div>
    <div class="figure" style="margin-top: 0;">
      <div class="figure-frame" onclick="openLightbox(this)">
        <img src="img/fig7_v2.png" alt="Figure 7: Chess OOD">
      </div>
      <div class="figure-caption"><strong>Figure 7.</strong> Reverse 순서(높은 epiplexity)로 학습한 모델이 두 OOD task 모두에서 우수. 풍부한 보드 상태 표현이 transfer에 유리함.</div>
    </div>
  </div>

  <div class="callout callout-orange reveal">
    <div class="callout-icon">&#128204;</div>
    <div><strong>주의:</strong> 높은 epiplexity는 더 많은 구조를 학습했다는 의미이지, 특정 downstream task 성능을 <em>보장</em>하지는 않는다. Epiplexity는 구조의 <strong>양</strong>을 측정하지만, 그 구조가 특정 task에 관련되는지는 별개의 문제이다.</div>
  </div>
</section>


<!-- ══════════ 19. SCALING & DATA SELECTION ══════════ -->
<section class="section" id="sec19">
  <div class="reveal">
    <div class="section-number">19 — Application</div>
    <span class="section-tag tag-application">Application</span>
    <h2 class="section-title">자연 데이터의 Epiplexity & 데이터 선택</h2>
    <p class="section-subtitle">Section 6.2-6.4 — 텍스트 vs 이미지, VQ 토큰화, ADO 데이터 선택</p>
  </div>

  <h3 class="block-title reveal">자연 데이터의 정보 분해</h3>
  <div class="card-grid reveal reveal-delay-1" style="grid-template-columns: repeat(3, 1fr);">
    <div class="card" style="--card-accent: var(--accent-blue);">
      <h4>OpenWebText (텍스트)</h4>
      <p>\(S_T\) <strong>최대</strong>. 언어의 문법, 의미, 추론 등 풍부한 구조. 총 정보 대비 구조 비율이 높다.</p>
    </div>
    <div class="card" style="--card-accent: var(--accent-purple);">
      <h4>Lichess (체스)</h4>
      <p>\(S_T\) 중간. 전략적 구조가 있지만 텍스트보다는 제한적.</p>
    </div>
    <div class="card" style="--card-accent: var(--accent-pink);">
      <h4>CIFAR-5M (이미지)</h4>
      <p>총 정보량은 <strong>가장 많지만</strong> \(S_T\) 최소. 99%+ 정보가 랜덤 (픽셀 노이즈).</p>
    </div>
  </div>

  <p class="reveal reveal-delay-2">이미지를 <strong>VQ 토큰</strong>으로 변환하면 epiplexity가 크게 증가 — 모델이 저수준 픽셀 대신 <span class="hl hl-blue">고수준 의미 구조에 집중</span>할 수 있기 때문이다. 비디오는 시간 축 중복으로 이미지보다 더 낮은 epiplexity.</p>

  <div class="figure reveal reveal-delay-2">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig8_v2.png" alt="Figure 8: Natural data">
    </div>
    <div class="figure-caption"><strong>Figure 8.</strong> (a) Requential coding으로 측정한 자연 데이터의 epiplexity. 텍스트 >> 체스 >> 이미지. (b) Scaling law 기반 추정 (1T 토큰, \(10^{25}\) FLOPs). VQ 토큰화가 이미지 epiplexity를 크게 높임. (c) ADO로 선택한 데이터가 더 높은 epiplexity와 downstream 성능.</div>
  </div>

  <h3 class="block-title reveal">ADO: 데이터 선택과 Epiplexity</h3>
  <p class="reveal">Adaptive Data Optimization (ADO, Jiang et al. 2025)은 loss가 빠르게 감소하는 데이터 서브셋을 온라인으로 선호한다. 이는 결과적으로 loss 곡선 아래 면적(= prequential epiplexity)이 큰 데이터를 선택하는 것과 같다.</p>

  <div class="callout callout-key reveal">
    <div class="callout-icon">&#128204;</div>
    <div>
      <strong>데이터 선택 원칙 (Data Selection Principles):</strong>
      <ul class="clean-list accent-blue" style="margin-top: 0.5rem;">
        <li>총 바이트 수가 아니라 <strong>구조적 정보(Epiplexity)</strong>가 높은 데이터를 선택</li>
        <li>Loss가 꾸준히 감소하는 데이터 = 높은 epiplexity의 신호</li>
        <li>합성 데이터도 역함수가 어려운 변환을 사용하면 유용</li>
        <li>VQ 등 적절한 토큰화로 이미지/비디오 데이터의 epiplexity를 높일 수 있음</li>
      </ul>
    </div>
  </div>

  <div class="figure reveal">
    <div class="figure-frame" onclick="openLightbox(this)">
      <img src="img/fig9_v2.png" alt="Figure 9: Scaling law epiplexity">
    </div>
    <div class="figure-caption"><strong>Figure 9.</strong> Scaling law를 통해 추정한 epiplexity와 최적 학습 토큰 수. 데이터 양이 고정되면 compute가 무한히 커져도 epiplexity는 유한한 값에 수렴한다.</div>
  </div>
</section>


<!-- ══════════ 20. DISCUSSION ══════════ -->
<section class="section" id="sec20">
  <div class="reveal">
    <div class="section-number">20 — Discussion</div>
    <span class="section-tag tag-core">Wrap-up</span>
    <h2 class="section-title">정리 & 토론 포인트</h2>
  </div>

  <h3 class="block-title reveal">핵심 메시지 요약</h3>
  <table class="data-table reveal reveal-delay-1">
    <tr><th>질문</th><th>기존 이론</th><th>이 논문 (Epiplexity)</th></tr>
    <tr><td>정보를 만들 수 있는가?</td><td><span class="hl hl-red">No</span> (DPI)</td><td><span class="hl hl-green">Yes</span> — 역함수가 어려운 변환으로 (Thm 12)</td></tr>
    <tr><td>순서가 중요한가?</td><td><span class="hl hl-red">No</span> (Symmetry)</td><td><span class="hl hl-green">Yes</span> — OWP로 비대칭 (Thm 13)</td></tr>
    <tr><td>생성 과정 이상 배울 수 있나?</td><td><span class="hl hl-red">No</span> (Distribution matching)</td><td><span class="hl hl-green">Yes</span> — 귀납 + 창발 (Sec 5.3)</td></tr>
    <tr><td>합성 데이터의 가치?</td><td><span class="hl hl-red">DPI에 의해 없음</span></td><td><span class="hl hl-green">역함수 어려우면 있음</span> (Thm 12)</td></tr>
    <tr><td>텍스트 vs 이미지?</td><td>설명 불가</td><td><span class="hl hl-green">텍스트가 압도적 높은 \(S_T\)</span></td></tr>
  </table>

  <div class="two-col reveal reveal-delay-2" style="margin-top: 1.5rem;">
    <div>
      <div class="callout callout-green">
        <div class="callout-icon">&#10003;</div>
        <div>
          <strong>핵심 기여</strong>
          <ul class="clean-list accent-green" style="margin-top: 0.5rem;">
            <li>Epiplexity: 계산 제약 하 추출 가능 구조적 정보 형식화</li>
            <li>세 역설을 이론(Thm 9, 10, 12, 13) + 실험으로 해소</li>
            <li>Prequential/Requential 측정 방법 제안</li>
            <li>"Model Selection" → <strong>"Data Selection"</strong> 패러다임 전환</li>
            <li>자연 데이터 간 구조적 정보 비교 프레임워크</li>
          </ul>
        </div>
      </div>
    </div>
    <div>
      <div class="callout callout-orange">
        <div class="callout-icon">&#9888;</div>
        <div>
          <strong>한계 & 열린 문제</strong>
          <ul class="clean-list accent-pink" style="margin-top: 0.5rem;">
            <li>높은 \(S_T\) ≠ 특정 task 성능 보장</li>
            <li>이론적 하한 \(\Omega(\log n)\) vs 실제 power law 스케일링</li>
            <li>측정에 대규모 모델 학습이 필요 (고비용)</li>
            <li>poly vs non-poly 외 세밀한 계산 제약 탐구 필요</li>
            <li>메모리 제약, 아키텍처 특화 분석 미포함</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <h3 class="block-title reveal" style="margin-top: 2rem;">세미나 토론 포인트</h3>
  <div class="card reveal reveal-delay-1" style="--card-accent: var(--accent-blue);">
    <ul class="clean-list accent-blue">
      <li>Epiplexity를 실제 LLM 학습 파이프라인의 데이터 선택에 어떻게 적용할 수 있을까?</li>
      <li>계산 제약 \(T\)의 선택 기준은? — 실제 학습 FLOP? 아키텍처 특성?</li>
      <li>Chain-of-thought는 Emergence 관점에서 "T를 늘려 brute-force에 접근"하는 것인가?</li>
      <li>이미지 데이터의 epiplexity를 높이려면 어떤 전처리/토크나이제이션이 효과적인가?</li>
      <li>Scaling law exponent(\(\beta\))와 데이터의 epiplexity 사이의 관계는?</li>
      <li>멀티모달 학습에서 각 모달리티의 epiplexity를 균형 있게 조합하는 전략은?</li>
      <li>RLHF/PPO 단계에서 생성되는 합성 데이터의 epiplexity는 어떻게 해석하나?</li>
    </ul>
  </div>
</section>

</div><!-- .page -->

<footer>
  <div class="footer-inner">
    <div class="footer-left">
      Paper Seminar: "From Entropy to Epiplexity" (Finzi et al., 2026)
    </div>
    <div class="footer-right">
      Prepared for Lab Seminar
    </div>
  </div>
</footer>

<button class="scroll-top" id="scrollTop" onclick="window.scrollTo({top:0,behavior:'smooth'})">&#8593;</button>

<!-- ═══════════════════════════════════════════
     JAVASCRIPT
     ═══════════════════════════════════════════ -->
<script>
document.addEventListener('DOMContentLoaded', function() {

  // KaTeX auto-render
  if (typeof renderMathInElement !== 'undefined') {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '\\[', right: '\\]', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false}
      ],
      throwOnError: false
    });
  }

  // Reading Progress Bar
  const progressBar = document.getElementById('progressBar');
  function updateProgress() {
    const scrollTop = window.scrollY;
    const docHeight = document.documentElement.scrollHeight - window.innerHeight;
    const progress = (scrollTop / docHeight) * 100;
    progressBar.style.width = progress + '%';
  }

  // Scroll-to-top button
  const scrollTopBtn = document.getElementById('scrollTop');
  function updateScrollTop() {
    scrollTopBtn.style.display = window.scrollY > 600 ? 'flex' : 'none';
  }

  // Nav dots visibility
  const navDots = document.getElementById('navDots');
  function updateNavDots() {
    navDots.classList.toggle('visible', window.scrollY > window.innerHeight * 0.5);
  }

  // Throttled scroll handler
  let ticking = false;
  window.addEventListener('scroll', function() {
    if (!ticking) {
      window.requestAnimationFrame(function() {
        updateProgress();
        updateScrollTop();
        updateNavDots();
        ticking = false;
      });
      ticking = true;
    }
  });

  // Scroll Reveal via IntersectionObserver
  const reveals = document.querySelectorAll('.reveal');
  const revealObserver = new IntersectionObserver(function(entries) {
    entries.forEach(function(entry) {
      if (entry.isIntersecting) {
        entry.target.classList.add('visible');
      }
    });
  }, { threshold: 0.1, rootMargin: '0px 0px -40px 0px' });

  reveals.forEach(function(el) { revealObserver.observe(el); });

  // Active nav dot tracking
  const sections = document.querySelectorAll('.section');
  const dotLinks = document.querySelectorAll('.nav-dots a');
  const sectionObserver = new IntersectionObserver(function(entries) {
    entries.forEach(function(entry) {
      if (entry.isIntersecting) {
        const id = entry.target.id;
        dotLinks.forEach(function(dot) {
          dot.classList.toggle('active', dot.dataset.section === id);
        });
      }
    });
  }, { threshold: 0.3, rootMargin: '-10% 0px -60% 0px' });

  sections.forEach(function(sec) { sectionObserver.observe(sec); });
});

// Lightbox
function openLightbox(el) {
  const img = el.querySelector('img');
  if (!img) return;
  const lb = document.getElementById('lightbox');
  document.getElementById('lightboxImg').src = img.src;
  lb.classList.add('open');
  document.body.style.overflow = 'hidden';
}
function closeLightbox() {
  document.getElementById('lightbox').classList.remove('open');
  document.body.style.overflow = '';
}
document.getElementById('lightbox').addEventListener('click', function(e) {
  if (e.target === this) closeLightbox();
});
document.addEventListener('keydown', function(e) {
  if (e.key === 'Escape') closeLightbox();
});
</script>
</body>
</html>
